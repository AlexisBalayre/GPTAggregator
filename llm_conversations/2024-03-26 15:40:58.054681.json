[
    {
        "content": "import os\nimport json\n\nimport ollama\nfrom openai import OpenAI\nfrom anthropic import Anthropic\nfrom mistralai.client import MistralClient\n\n\nclass LLMConnector:\n    \"\"\"\n    A connector class designed to interface with various large language models (LLMs) through their respective APIs.\n    Supports local and online model querying, as well as streaming responses from these models.\n\n    Parameters:\n        st (Streamlit): The Streamlit instance to use for logging and displaying messages.\n    \"\"\"\n\n    def __init__(self, st):\n        \"\"\"\n        Initializes the LLMConnector with client instances for each supported API and loads the configuration for online models.\n        \"\"\"\n        self.st = st\n\n        self.ollama_client = None  # Client for interacting with local Ollama models.\n        self.openai_client = None  # Client for OpenAI's API.\n        self.anthropic_client = None  # Client for Anthropic's API.\n        self.mistralai_client = None  # Client for MistralAI's API.\n\n        self.__init_ollama()\n        self.__init_openai()\n        self.__init_anthropic()\n        self.__init_mistralai()\n\n        self.online_models = json.load(\n            open(\"models.json\")\n        )  # Loads the online model configurations from a JSON file.\n\n    def __init_ollama(self):\n        \"\"\"\n        Initializes the Ollama client for interacting with local models.\n        \"\"\"\n        try:\n            self.ollama_client = ollama\n            # Count the number of available models.\n            model_count = len(self.ollama_client.list()[\"models\"])\n            if model_count == 0:\n                # Display a warning message if no models are available.\n                self.st.warning(\"No local models found.\")\n        except Exception as e:\n            # Display a warning message if the Ollama client fails to initialize.\n            self.st.warning(\"Failed to initialize Ollama client.\")\n            print(f\"Failed to initialize Ollama client: {e}\")\n\n    def __init_openai(self):\n        \"\"\"\n        Initializes the OpenAI client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.openai_client = OpenAI()\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize OpenAI client: {e}\")\n\n    def __init_mistralai(self):\n        \"\"\"\n        Initializes the MistralAI client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.mistralai_client = MistralClient(\n                api_key=os.getenv(\"MISTRALAI_API_KEY\")\n            )\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No MistralAI API key found. Please set the MISTRALAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize MistralAI client: {e}\")\n\n    def __init_anthropic(self):\n        \"\"\"\n        Initializes the Anthropic client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.anthropic_client = Anthropic()\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No Anthropic API key found. Please set the ANTHROPIC_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize Anthropic client: {e}\")\n\n    def health_check(self):\n        \"\"\"\n        Performs a health check on the Ollama, OpenAI, Anthropic, and MistralAI clients to verify their availability.\n\n        Returns:\n            dict: A dictionary containing the health status of each client.\n        \"\"\"\n        ollama_check = False\n        openai_check = False\n        anthropic_check = False\n        mistralai_check = False\n\n        if self.ollama_client:\n            ollama_check = True\n        if self.openai_client:\n            openai_check = True\n        if self.anthropic_client:\n            anthropic_check = True\n        if self.mistralai_client:\n            mistralai_check = True\n\n        return {\n            \"ollama\": ollama_check,\n            \"openai\": openai_check,\n            \"anthropic\": anthropic_check,\n            \"mistralai\": mistralai_check,\n        }\n\n    def get_local_models(self):\n        \"\"\"\n        Fetches and returns a list of local models available through the Ollama client.\n\n        Returns:\n            list: A list of model names available through the Ollama client.\n        \"\"\"\n        return self.ollama_client.list()[\"models\"]\n\n    def get_online_models(self):\n        \"\"\"\n        Fetches and returns a list of online models available through the Ollama, OpenAI, Anthropic, and MistralAI clients.\n\n        Returns:\n            dict: A dictionary of online model configurations.\n        \"\"\"\n        return self.online_models\n\n    def llm_stream(self, provider, model_name, messages):\n        \"\"\"\n        Streams responses from the specified language model in real-time. This method handles interactions with different LLM providers by yielding messages as they become available.\n\n        Args:\n            provider (str): The name of the LLM provider (ollama, openai, anthropic, mistralai).\n            model_name (str): The name or identifier of the model to use for generating responses.\n            messages (list): A list of messages to send to the model. Can include questions or prompts.\n\n        Yields:\n            str: A response message generated by the language model.\n        \"\"\"\n\n        # Ollama API - Local LLM\n        if provider == \"ollama\":\n            stream = self.ollama_client.chat(model_name, messages, stream=True)\n            for chunk in stream:\n                yield chunk[\"message\"][\"content\"]\n\n        # OpenAI API\n        elif provider == \"openai\":\n            stream = self.openai_client.chat.completions.create(\n                model=model_name,\n                messages=messages,\n                stream=True,\n            )\n            for chunk in stream:\n                content = chunk.choices[0].delta.content\n                if content:\n                    yield content\n\n        # Anthropic API\n        elif provider == \"anthropic\":\n            with self.anthropic_client.messages.stream(\n                max_tokens=1024,\n                model=model_name,\n                messages=messages,\n            ) as stream:\n                for text in stream.text_stream:\n                    yield text\n\n        # MistralAI API\n        elif provider == \"mistralai\":\n            stream = self.mistralai_client.chat_stream(\n                model=model_name,\n                messages=messages,\n            )\n            for chunk in stream:\n                content = chunk.choices[0].delta.content\n                if content:\n                    yield content\n\n\ncorrect the health function",
        "role": "user"
    },
    {
        "content": " def health_check(self):\n    \"\"\"\n    Performs a health check on the Ollama, OpenAI, Anthropic, and MistralAI clients to verify their availability.\n\n    Returns:\n        dict: A dictionary containing the health status of each client.\n    \"\"\"\n    ollama_check = False\n    openai_check = False\n    anthropic_check = False\n    mistralai_check = False\n\n    if self.ollama_client:\n        try:\n            _ = self.ollama_client.list()[\"models\"]  # Perform a dummy request to ensure the client is accessible\n            ollama_check = True\n        except Exception as e:\n            print(f\"Failed to initialize Ollama client due to error: {e}\")\n\n    if self.openai_client:\n        try:\n            _ = self.openai_client.get_organization()  # Perform a dummy request to ensure the client is accessible\n            openai_check = True\n        except Exception as e:\n            print(f\"Failed to initialize OpenAI client due to error: {e}\")\n\n    if self.anthropic_client:\n        try:\n            _ = self.anthropic_client.get_models()  # Perform a dummy request to ensure the client is accessible\n            anthropic_check = True\n        except Exception as e:\n            print(f\"Failed to initialize Anthropic client due to error: {e}\")\n\n    if self.mistralai_client:\n        try:\n            _ = self.mistralai_client.get_models()  # Perform a dummy request to ensure the client is accessible\n            mistralai_check = True\n        except Exception as e:\n            print(f\"Failed to initialize MistralAI client due to error: {e}\")\n\n    return {\n        \"ollama\": ollama_check,\n        \"openai\": openai_check,\n        \"anthropic\": anthropic_check,\n        \"mistralai\": mistralai_check,\n    }",
        "role": "assistant"
    },
    {
        "content": "Give me the corrected code",
        "role": "user"
    },
    {
        "content": "Here's the corrected code for the `LLMConnector` class:\n\n```python\nimport os\nimport json\n\nimport ollama\nfrom openai import OpenAI\nfrom anthropic import Anthropic\nfrom mistralai.client import MistralClient\n\n\nclass LLMConnector:\n    \"\"\"\n    A connector class designed to interface with various large language models (LLMs) through their respective APIs.\n    Supports local and online model querying, as well as streaming responses from these models.\n\n    Parameters:\n        st (Streamlit): The Streamlit instance to use for logging and displaying messages.\n    \"\"\"\n\n    def __init__(self, st):\n        \"\"\"\n        Initializes the LLMConnector with client instances for each supported API and loads the configuration for online models.\n        \"\"\"\n        self.st = st\n\n        self.ollama_client = None  # Client for interacting with local Ollama models.\n        self.openai_client = None  # Client for OpenAI's API.\n        self.anthropic_client = None  # Client for Anthropic's API.\n        self.mistralai_client = None  # Client for MistralAI's API.\n\n        self.__init_ollama()\n        self.__init_openai()\n        self.__init_anthropic()\n        self.__init_mistralai()\n\n        self.online_models = json.load(\n            open(\"models.json\")\n        )  # Loads the online model configurations from a JSON file.\n\n    def __init_ollama(self):\n        \"\"\"\n        Initializes the Ollama client for interacting with local models.\n        \"\"\"\n        try:\n            self.ollama_client = ollama\n            # Count the number of available models.\n            model_count = len(self.ollama_client.list()[\"models\"])\n            if model_count == 0:\n                # Display a warning message if no models are available.\n                self.st.warning(\"No local models found.\")\n        except Exception as e:\n            # Display a warning message if the Ollama client fails to initialize.\n            self.st.warning(\"Failed to initialize Ollama client.\")\n            print(f\"Failed to initialize Ollama client: {e}\")\n\n    def __init_openai(self):\n        \"\"\"\n        Initializes the OpenAI client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.openai_client = OpenAI()\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize OpenAI client: {e}\")\n\n    def __init_mistralai(self):\n        \"\"\"\n        Initializes the MistralAI client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.mistralai_client = MistralClient(\n                api_key=os.getenv(\"MISTRALAI_API_KEY\")\n            )\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No MistralAI API key found. Please set the MISTRALAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize MistralAI client: {e}\")\n\n    def __init_anthropic(self):\n        \"\"\"\n        Initializes the Anthropic client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.anthropic_client = Anthropic()\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No Anthropic API key found. Please set the ANTHROPIC_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize Anthropic client: {e}\")\n\n    def health_check(self):\n        \"\"\"\n        Performs a health check on the Ollama, OpenAI, Anthropic, and MistralAI clients to verify their availability.\n\n        Returns:\n            dict: A dictionary containing the health status of each client.\n        \"\"\"\n        ollama_check = False\n        openai_check = False\n        anthropic_check = False",
        "role": "assistant"
    },
    {
        "content": "Correct this code",
        "role": "user"
    },
    {
        "content": "Here's the corrected code for the `LLMConnector` class:\n\n```python\nimport os\nimport json\n\nimport ollama\nfrom openai import OpenAI\nfrom anthropic import Anthropic\nfrom mistralai.client import MistralClient\n\n\nclass LLMConnector:\n    \"\"\"\n    A connector class designed to interface with various large language models (LLMs) through their respective APIs.\n    Supports local and online model querying, as well as streaming responses from these models.\n\n    Parameters:\n        st (Streamlit): The Streamlit instance to use for logging and displaying messages.\n    \"\"\"\n\n    def __init__(self, st):\n        \"\"\"\n        Initializes the LLMConnector with client instances for each supported API and loads the configuration for online models.\n        \"\"\"\n        self.st = st\n\n        self.ollama_client = None  # Client for interacting with local Ollama models.\n        self.openai_client = None  # Client for OpenAI's API.\n        self.anthropic_client = None  # Client for Anthropic's API.\n        self.mistralai_client = None  # Client for MistralAI's API.\n\n        self.__init_ollama()\n        self.__init_openai()\n        self.__init_anthropic()\n        self.__init_mistralai()\n\n        self.online_models = json.load(\n            open(\"models.json\")\n        )  # Loads the online model configurations from a JSON file.\n\n    def __init_ollama(self):\n        \"\"\"\n        Initializes the Ollama client for interacting with local models.\n        \"\"\"\n        try:\n            self.ollama_client = ollama\n            # Count the number of available models.\n            model_count = len(self.ollama_client.list()[\"models\"])\n            if model_count == 0:\n                # Display a warning message if no models are available.\n                self.st.warning(\"No local models found.\")\n        except Exception as e:\n            # Display a warning message if the Ollama client fails to initialize.\n            self.st.warning(\"Failed to initialize Ollama client.\")\n            print(f\"Failed to initialize Ollama client: {e}\")\n\n    def __init_openai(self):\n        \"\"\"\n        Initializes the OpenAI client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.openai_client = OpenAI()\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize OpenAI client: {e}\")\n\n    def __init_mistralai(self):\n        \"\"\"\n        Initializes the MistralAI client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.mistralai_client = MistralClient(\n                api_key=os.getenv(\"MISTRALAI_API_KEY\")\n            )\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No MistralAI API key found. Please set the MISTRALAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize MistralAI client: {e}\")\n\n    def __init_anthropic(self):\n        \"\"\"\n        Initializes the Anthropic client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.anthropic_client = Anthropic()\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No Anthropic API key found. Please set the ANTHROPIC_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize Anthropic client: {e}\")\n\n    def health_check(self):\n        \"\"\"\n        Performs a health check on the Ollama, OpenAI, Anthropic, and MistralAI clients to verify their availability.\n\n        Returns:\n            dict: A dictionary containing the health status of each client.\n        \"\"\"\n        ollama_check = False\n        openai_check = False\n        anthropic_check = False",
        "role": "assistant"
    },
    {
        "content": "import os\nimport json\n\nimport ollama\nfrom openai import OpenAI\nfrom anthropic import Anthropic\nfrom mistralai.client import MistralClient\n\n\nclass LLMConnector:\n    \"\"\"\n    A connector class designed to interface with various large language models (LLMs) through their respective APIs.\n    Supports local and online model querying, as well as streaming responses from these models.\n\n    Parameters:\n        st (Streamlit): The Streamlit instance to use for logging and displaying messages.\n    \"\"\"\n\n    def __init__(self, st):\n        \"\"\"\n        Initializes the LLMConnector with client instances for each supported API and loads the configuration for online models.\n        \"\"\"\n        self.st = st\n\n        self.ollama_client = None  # Client for interacting with local Ollama models.\n        self.openai_client = None  # Client for OpenAI's API.\n        self.anthropic_client = None  # Client for Anthropic's API.\n        self.mistralai_client = None  # Client for MistralAI's API.\n\n        self.__init_ollama()\n        self.__init_openai()\n        self.__init_anthropic()\n        self.__init_mistralai()\n\n        self.online_models = json.load(\n            open(\"models.json\")\n        )  # Loads the online model configurations from a JSON file.\n\n    def __init_ollama(self):\n        \"\"\"\n        Initializes the Ollama client for interacting with local models.\n        \"\"\"\n        try:\n            self.ollama_client = ollama\n            # Count the number of available models.\n            model_count = len(self.ollama_client.list()[\"models\"])\n            if model_count == 0:\n                # Display a warning message if no models are available.\n                self.st.warning(\"No local models found.\")\n        except Exception as e:\n            # Display a warning message if the Ollama client fails to initialize.\n            self.st.warning(\"Failed to initialize Ollama client.\")\n            print(f\"Failed to initialize Ollama client: {e}\")\n\n    def __init_openai(self):\n        \"\"\"\n        Initializes the OpenAI client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.openai_client = OpenAI()\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize OpenAI client: {e}\")\n\n    def __init_mistralai(self):\n        \"\"\"\n        Initializes the MistralAI client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.mistralai_client = MistralClient(\n                api_key=os.getenv(\"MISTRALAI_API_KEY\")\n            )\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No MistralAI API key found. Please set the MISTRALAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize MistralAI client: {e}\")\n\n    def __init_anthropic(self):\n        \"\"\"\n        Initializes the Anthropic client with the API key stored in the environment variables.\n        \"\"\"\n        try:\n            self.anthropic_client = Anthropic()\n        except Exception as e:\n            # Display a warning message if the API key is missing.\n            self.st.warning(\n                \"No Anthropic API key found. Please set the ANTHROPIC_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize Anthropic client: {e}\")\n\n    def health_check(self):\n        \"\"\"\n        Performs a health check on the Ollama, OpenAI, Anthropic, and MistralAI clients to verify their availability.\n\n        Returns:\n            dict: A dictionary containing the health status of each client.\n        \"\"\"\n        ollama_check = False\n        openai_check = False\n        anthropic_check = False\n        mistralai_check = False\n\n        if self.ollama_client:\n            ollama_check = True\n        if self.openai_client:\n            openai_check = True\n        if self.anthropic_client:\n            anthropic_check = True\n        if self.mistralai_client:\n            mistralai_check = True\n\n        return {\n            \"ollama\": ollama_check,\n            \"openai\": openai_check,\n            \"anthropic\": anthropic_check,\n            \"mistralai\": mistralai_check,\n        }\n\n    def get_local_models(self):\n        \"\"\"\n        Fetches and returns a list of local models available through the Ollama client.\n\n        Returns:\n            list: A list of model names available through the Ollama client.\n        \"\"\"\n        return self.ollama_client.list()[\"models\"]\n\n    def get_online_models(self):\n        \"\"\"\n        Fetches and returns a list of online models available through the Ollama, OpenAI, Anthropic, and MistralAI clients.\n\n        Returns:\n            dict: A dictionary of online model configurations.\n        \"\"\"\n        return self.online_models\n\n    def llm_stream(self, provider, model_name, messages):\n        \"\"\"\n        Streams responses from the specified language model in real-time. This method handles interactions with different LLM providers by yielding messages as they become available.\n\n        Args:\n            provider (str): The name of the LLM provider (ollama, openai, anthropic, mistralai).\n            model_name (str): The name or identifier of the model to use for generating responses.\n            messages (list): A list of messages to send to the model. Can include questions or prompts.\n\n        Yields:\n            str: A response message generated by the language model.\n        \"\"\"\n\n        # Ollama API - Local LLM\n        if provider == \"ollama\":\n            stream = self.ollama_client.chat(model_name, messages, stream=True)\n            for chunk in stream:\n                yield chunk[\"message\"][\"content\"]\n\n        # OpenAI API\n        elif provider == \"openai\":\n            stream = self.openai_client.chat.completions.create(\n                model=model_name,\n                messages=messages,\n                stream=True,\n            )\n            for chunk in stream:\n                content = chunk.choices[0].delta.content\n                if content:\n                    yield content\n\n        # Anthropic API\n        elif provider == \"anthropic\":\n            with self.anthropic_client.messages.stream(\n                max_tokens=1024,\n                model=model_name,\n                messages=messages,\n            ) as stream:\n                for text in stream.text_stream:\n                    yield text\n\n        # MistralAI API\n        elif provider == \"mistralai\":\n            stream = self.mistralai_client.chat_stream(\n                model=model_name,\n                messages=messages,\n            )\n            for chunk in stream:\n                content = chunk.choices[0].delta.content\n                if content:\n                    yield content\n\ncorrect the code, don't write the comments",
        "role": "user"
    },
    {
        "content": "Here's the corrected code for the `LLMConnector` class without the comments:\n\n```python\nimport os\nimport json\n\nimport ollama\nfrom openai import OpenAI\nfrom anthropic import Anthropic\nfrom mistralai.client import MistralClient\n\n\nclass LLMConnector:\n    def __init__(self, st):\n        self.st = st\n\n        self.ollama_client = None\n        self.openai_client = None\n        self.anthropic_client = None\n        self.mistralai_client = None\n\n        self.__init_ollama()\n        self.__init_openai()\n        self.__init_anthropic()\n        self.__init_mistralai()\n\n        self.online_models = json.load(open(\"models.json\"))\n\n    def __init_ollama(self):\n        try:\n            self.ollama_client = ollama\n            model_count = len(self.ollama_client.list()[\"models\"])\n            if model_count == 0:\n                self.st.warning(\"No local models found.\")\n        except Exception as e:\n            self.st.warning(\"Failed to initialize Ollama client.\")\n            print(f\"Failed to initialize Ollama client: {e}\")\n\n    def __init_openai(self):\n        try:\n            self.openai_client = OpenAI()\n        except Exception as e:\n            self.st.warning(\n                \"No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize OpenAI client: {e}\")\n\n    def __init_mistralai(self):\n        try:\n            self.mistralai_client = MistralClient(api_key=os.getenv(\"MISTRALAI_API_KEY\"))\n        except Exception as e:\n            self.st.warning(\n                \"No MistralAI API key found. Please set the MISTRALAI_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize MistralAI client: {e}\")\n\n    def __init_anthropic(self):\n        try:\n            self.anthropic_client = Anthropic()\n        except Exception as e:\n            self.st.warning(\n                \"No Anthropic API key found. Please set the ANTHROPIC_API_KEY environment variable.\"\n            )\n            print(f\"Failed to initialize Anthropic client: {e}\")\n\n    def health_check(self):\n        ollama_check = False\n        openai_check = False\n        anthropic_check = False\n        mistralai_check = False\n\n        if self.ollama_client:\n            try:\n                _ = self.ollama_client.list()[\"models\"]\n                ollama_check = True\n            except Exception as e:\n                print(f\"Failed to initialize Ollama client due to error: {e}\")\n\n        if self.openai_client:\n            try:\n                _ = self.openai_client.get_organization()\n                openai_check = True\n            except Exception as e:\n                print(f\"Failed to initialize OpenAI client due to error: {e}\")\n\n        if self.anthropic_client:\n            try:\n                _ = self.anthropic_client.get_models()\n                anthropic_check = True\n            except Exception as e:\n                print(f\"Failed to initialize Anthropic client due to error: {e}\")\n\n        if self.mistralai_client:\n            try:\n                _ = self.mistralai_client.get_models()\n                mistralai_check = True\n            except Exception as e:\n                print(f\"Failed to initialize MistralAI client due to error: {e}\")\n\n        return {\n            \"ollama\": ollama_check,\n            \"openai\": openai_check,\n            \"anthropic\": anthropic_check,\n            \"mistralai\": mistralai_check,\n        }\n\n    def get_local_models(self):\n        return self.ollama_client.list()[\"models\"]\n\n    def get_online_models(self):\n        return self.online_models\n\n    def llm_stream(self, provider, model_name, messages):\n        if provider == \"ollama\":\n            stream = self.ollama_client.chat(model_name,",
        "role": "assistant"
    }
]