[
    {
        "content": "import os\nimport json\nimport datetime\nfrom uuid import uuid4\n\nimport streamlit as st\nfrom LLMConnector import LLMConnector\n\n\nclass Chat:\n    def __init__(self):\n        self.llmConnector = LLMConnector()\n        self.LOCAL_MODELS = self.llmConnector.get_local_models()\n        self.ONLINE_MODELS = self.llmConnector.get_online_models()\n\n        self.OUTPUT_DIR = \"llm_conversations\"\n\n        self.st = st\n        self.__configure_chat()\n        self.selected_model = self.select_model()\n\n    def __configure_chat(self):\n        self.st.set_page_config(\n            layout=\"wide\", page_title=\"GPTAggregator\", page_icon=\"\ud83d\ude80\"\n        )\n        self.st.sidebar.title(\"GPTAggregator \ud83d\ude80\")\n        self.display_conversation_history()\n        self.new_conversation()\n\n    def run(self):\n        prompt = self.st.chat_input(f\"Ask '{self.selected_model}' a question ...\")\n\n        self.st.session_state[\"conversation_id\"] = self.st.session_state.get(\n            \"conversation_id\", None\n        )\n\n        self.st.session_state[\n            \"chat_history_\" + self.st.session_state[\"conversation_id\"]\n        ] = self.st.session_state.get(\n            \"chat_history_\" + self.st.session_state[\"conversation_id\"], []\n        )\n\n        self.chat(prompt)\n\n    def new_conversation(self):\n        new_conversation = self.st.sidebar.button(\n            \"New conversation\", key=\"new_conversation\"\n        )\n        if new_conversation:\n            self.st.session_state[\"conversation_id\"] = str(uuid4())\n            self.st.session_state[\"chat_history_\" + self.st.session_state[\"conversation_id\"]] = []\n            self.st.rerun()\n\n    def select_model(self):\n        local_models_names = [\n            model[\"name\"] for model in self.LOCAL_MODELS\n        ]  # Local models\n        online_models_names = [\n            model[\"name\"] for model in self.ONLINE_MODELS\n        ]  # Online models\n        model_names = local_models_names + online_models_names  # All models\n\n        # Sidebar - Model selection\n        self.st.sidebar.subheader(\"Models\")\n        llm_name = self.st.sidebar.selectbox(\n            f\"Select Model (available {len(model_names)})\", model_names\n        )\n\n        # Model details\n        if llm_name:\n            # local model\n            if llm_name in local_models_names:\n                llm_details = [\n                    model for model in self.LOCAL_MODELS if model[\"name\"] == llm_name\n                ][0]\n                if type(llm_details[\"size\"]) != str:\n                    llm_details[\"size\"] = f\"{round(llm_details['size'] / 1e9, 2)} GB\"\n                llm_provider = \"ollama\"\n\n            # online model\n            else:\n                llm_details = [\n                    model for model in self.ONLINE_MODELS if model[\"name\"] == llm_name\n                ][0]\n                llm_provider = llm_details[\"provider\"]\n                llm_name = llm_details[\"modelName\"]\n\n            with self.st.expander(\"LLM Details\"):\n                self.st.write(llm_details)\n\n            return [llm_provider, llm_name]\n\n        return None\n\n    def display_conversation_history(self):\n        OUTPUT_DIR = os.path.join(os.getcwd(), self.OUTPUT_DIR)\n\n        # Get a list of all conversation files for the selected model\n        conversation_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".json\")]\n\n        # Sidebar - Conversation history\n        self.st.sidebar.subheader(\"Conversation History\")\n        selected_conversation = self.st.sidebar.selectbox(\n            \"Select a conversation\", conversation_files, index=0\n        )\n\n        if selected_conversation:\n            # Load the conversation data from the selected file\n            conversation_file = os.path.join(OUTPUT_DIR, selected_conversation)\n            with open(conversation_file, \"r\") as f:\n                conversation_data = json.load(f)\n\n            self.st.session_state[\"conversation_id\"] = selected_conversation.split(\"_\")[\n                0\n            ]\n\n            # Display the conversation in the main area\n            for message in conversation_data:\n                role = message[\"role\"]\n                if role == \"user\":\n                    with self.st.chat_message(\"user\"):\n                        question = message[\"content\"]\n                        self.st.markdown(f\"{question}\", unsafe_allow_html=True)\n                elif role == \"assistant\":\n                    with self.st.chat_message(\"assistant\"):\n                        self.st.markdown(message[\"content\"], unsafe_allow_html=True)\n\n    def chat(self, prompt):\n        if self.st.session_state[\"conversation_id\"]:\n            chat_history_key = (\n                f\"chat_history_{self.st.session_state['conversation_id']}\"\n            )\n        else:\n            # Create a new conversation\n            self.st.session_state[\"conversation_id\"] = str(uuid4())\n            chat_history_key = (\n                f\"chat_history_{self.st.session_state['conversation_id']}\"\n            )\n            self.st.session_state[chat_history_key] = []\n\n        # Display chat history\n        for message in self.st.session_state[chat_history_key]:\n            role = message[\"role\"]\n            if role == \"user\":\n                with self.st.chat_message(\"user\"):\n                    question = message[\"content\"]\n                    self.st.markdown(f\"{question}\", unsafe_allow_html=True)\n            elif role == \"assistant\":\n                with self.st.chat_message(\"assistant\"):\n                    self.st.markdown(message[\"content\"], unsafe_allow_html=True)\n\n        # run the model\n        if prompt:\n            self.st.session_state[chat_history_key].append(\n                {\"content\": f\"{prompt}\", \"role\": \"user\"}\n            )\n            with self.st.chat_message(\"user\"):\n                self.st.write(prompt)\n\n            messages = [\n                dict(content=message[\"content\"], role=message[\"role\"])\n                for message in self.st.session_state[chat_history_key]\n            ]\n\n            with self.st.chat_message(\"assistant\"):\n                chat_box = self.st.empty()\n                response_message = chat_box.write_stream(\n                    self.llmConnector.llm_stream(\n                        self.selected_model[0], self.selected_model[1], messages\n                    )\n                )\n\n            self.st.session_state[chat_history_key].append(\n                {\"content\": f\"{response_message}\", \"role\": \"assistant\"}\n            )\n\n            self.save_conversation()\n\n            return response_message\n\n    def save_conversation(self):\n        conversation_id = self.st.session_state[\"conversation_id\"]\n        conversation_key = f\"chat_history_{conversation_id}\"\n        conversation_chat = self.st.session_state[conversation_key]\n        filename = f\"{conversation_id}_{self.selected_model[1]}.json\"\n        if conversation_chat:\n            conversation_file = os.path.join(self.OUTPUT_DIR, filename)\n            if os.path.exists(conversation_file):\n                with open(conversation_file, \"r\") as f:\n                    existing_conversation_data = json.load(f)\n                conversation_chat.extend(existing_conversation_data)\n            with open(conversation_file, \"w\") as f:\n                json.dump(conversation_chat, f, indent=4)\n                self.st.success(f\"Conversation saved to {conversation_file}\")\n\ncorrect this code",
        "role": "user"
    },
    {
        "content": "Here is the corrected code for the `Chat` class:\n\n```python\nimport os\nimport json\nimport datetime\nfrom uuid import uuid4\n\nimport streamlit as st\nfrom LLMConnector import LLMConnector\n\n\nclass Chat:\n    def __init__(self):\n        self.llmConnector = LLMConnector()\n        self.LOCAL_MODELS = self.llmConnector.get_local_models()\n        self.ONLINE_MODELS = self.llmConnector.get_online_models()\n\n        self.OUTPUT_DIR = \"ll\n",
        "role": "assistant"
    },
    {
        "content": "how to make fire",
        "role": "user"
    },
    {
        "content": "**Materials:**\n\n* Tinder (dry, flammable material)\n* Kindling (small, dry pieces of wood)\n* Fuelwood (larger pieces of wood)\n* Matches or a lighter\n* Dry wood or other fire starter\n\n**Instructions:**\n\n1. **Gather your tinder and kindling:** Find dry, flammable material such as dry wood, leaves, or bark. Kindling are small pieces of wood that are easy to ignite.\n2. **Create a spark:** Use a match or lighter to ignite the tinder. You can also use a fire starter or natural methods to create a spark.\n3. **Add fuelwood:** Once the tinder is ignited, gradually add small pieces of fuelwood to the fire.\n4. **Build the fire:** Continue adding fuelwood until the fire is large enough to cook or warm yourself.\n5. **Control the fire:** Once the fire is established, you can control its size by adding or removing fuelwood.\n\n**Tips:**\n\n* **Start small:** Don't try to build a huge fire right away. Start with a small fire and gradually increase the size as you get more experience.\n* **Control the flames:** Keep an eye on the fire and make sure it is under control.\n* **Never leave a fire unattended:** Make sure that the fire is completely extinguished before you leave the area.\n* **Use caution:** Fire can be dangerous, so use caution and always supervise children around the fire.\n* **Respect nature:** Do not damage the environment while making fire.\n\n**Additional Notes:**\n\n* **Fire safety:** Always have a fire extinguisher or other fire-fighting tools available.\n* **Camping:** If you are camping, it is important to follow local fire regulations and guidelines.\n* **Building fires in urban areas:** In some urban areas, it may be prohibited to build fires without a permit.\n* **Cultural considerations:** In some cultures, fire is seen as a sacred element and should be treated with respect.",
        "role": "assistant"
    },
    {
        "content": "test",
        "role": "user"
    },
    {
        "content": "**Here is a summary of the instructions on how to make fire:**\n\n**Gather your tinder and kindling:** Find dry, flammable material and small pieces of wood.\n\n**Create a spark:** Use a match or lighter to ignite the tinder.\n\n**Add fuelwood:** Gradually add larger pieces of wood to the fire.\n\n**Build the fire:** Continue adding fuelwood until the fire is large enough.\n\n**Control the fire:** Control the size of the fire by adding or removing fuelwood.\n\n**Additional tips:**\n\n* Start small.\n* Control the flames.\n* Never leave a fire unattended.\n* Use caution.\n* Respect nature.",
        "role": "assistant"
    },
    {
        "content": "import os\nimport json\nimport datetime\nfrom uuid import uuid4\n\nimport streamlit as st\nfrom LLMConnector import LLMConnector\n\n\nclass Chat:\n    def __init__(self):\n        self.llmConnector = LLMConnector()\n        self.LOCAL_MODELS = self.llmConnector.get_local_models()\n        self.ONLINE_MODELS = self.llmConnector.get_online_models()\n\n        self.OUTPUT_DIR = \"llm_conversations\"\n\n        self.st = st\n        self.__configure_chat()\n        self.selected_model = self.select_model()\n\n    def __configure_chat(self):\n        self.st.set_page_config(\n            layout=\"wide\", page_title=\"GPTAggregator\", page_icon=\"\ud83d\ude80\"\n        )\n        self.st.sidebar.title(\"GPTAggregator \ud83d\ude80\")\n        self.display_conversation_history()\n        self.new_conversation()\n\n    def run(self):\n        prompt = self.st.chat_input(f\"Ask '{self.selected_model}' a question ...\")\n\n        self.st.session_state[\"conversation_id\"] = self.st.session_state.get(\n            \"conversation_id\", None\n        )\n\n        self.st.session_state[\n            \"chat_history_\" + self.st.session_state[\"conversation_id\"]\n        ] = self.st.session_state.get(\n            \"chat_history_\" + self.st.session_state[\"conversation_id\"], []\n        )\n\n        self.chat(prompt)\n\n    def new_conversation(self):\n        new_conversation = self.st.sidebar.button(\n            \"New conversation\", key=\"new_conversation\"\n        )\n        if new_conversation:\n            self.st.session_state[\"conversation_id\"] = str(uuid4())\n            self.st.session_state[\"chat_history_\" + self.st.session_state[\"conversation_id\"]] = []\n            self.st.rerun()\n\n    def select_model(self):\n        local_models_names = [\n            model[\"name\"] for model in self.LOCAL_MODELS\n        ]  # Local models\n        online_models_names = [\n            model[\"name\"] for model in self.ONLINE_MODELS\n        ]  # Online models\n        model_names = local_models_names + online_models_names  # All models\n\n        # Sidebar - Model selection\n        self.st.sidebar.subheader(\"Models\")\n        llm_name = self.st.sidebar.selectbox(\n            f\"Select Model (available {len(model_names)})\", model_names\n        )\n\n        # Model details\n        if llm_name:\n            # local model\n            if llm_name in local_models_names:\n                llm_details = [\n                    model for model in self.LOCAL_MODELS if model[\"name\"] == llm_name\n                ][0]\n                if type(llm_details[\"size\"]) != str:\n                    llm_details[\"size\"] = f\"{round(llm_details['size'] / 1e9, 2)} GB\"\n                llm_provider = \"ollama\"\n\n            # online model\n            else:\n                llm_details = [\n                    model for model in self.ONLINE_MODELS if model[\"name\"] == llm_name\n                ][0]\n                llm_provider = llm_details[\"provider\"]\n                llm_name = llm_details[\"modelName\"]\n\n            with self.st.expander(\"LLM Details\"):\n                self.st.write(llm_details)\n\n            return [llm_provider, llm_name]\n\n        return None\n\n    def display_conversation_history(self):\n        OUTPUT_DIR = os.path.join(os.getcwd(), self.OUTPUT_DIR)\n\n        # Get a list of all conversation files for the selected model\n        conversation_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".json\")]\n\n        # Sidebar - Conversation history\n        self.st.sidebar.subheader(\"Conversation History\")\n        selected_conversation = self.st.sidebar.selectbox(\n            \"Select a conversation\", conversation_files, index=0\n        )\n\n        if selected_conversation:\n            # Load the conversation data from the selected file\n            conversation_file = os.path.join(OUTPUT_DIR, selected_conversation)\n            with open(conversation_file, \"r\") as f:\n                conversation_data = json.load(f)\n\n            self.st.session_state[\"conversation_id\"] = selected_conversation.split(\"_\")[\n                0\n            ]\n\n            # Display the conversation in the main area\n            for message in conversation_data:\n                role = message[\"role\"]\n                if role == \"user\":\n                    with self.st.chat_message(\"user\"):\n                        question = message[\"content\"]\n                        self.st.markdown(f\"{question}\", unsafe_allow_html=True)\n                elif role == \"assistant\":\n                    with self.st.chat_message(\"assistant\"):\n                        self.st.markdown(message[\"content\"], unsafe_allow_html=True)\n\n    def chat(self, prompt):\n        if self.st.session_state[\"conversation_id\"]:\n            chat_history_key = (\n                f\"chat_history_{self.st.session_state['conversation_id']}\"\n            )\n        else:\n            # Create a new conversation\n            self.st.session_state[\"conversation_id\"] = str(uuid4())\n            chat_history_key = (\n                f\"chat_history_{self.st.session_state['conversation_id']}\"\n            )\n            self.st.session_state[chat_history_key] = []\n\n        # Display chat history\n        for message in self.st.session_state[chat_history_key]:\n            role = message[\"role\"]\n            if role == \"user\":\n                with self.st.chat_message(\"user\"):\n                    question = message[\"content\"]\n                    self.st.markdown(f\"{question}\", unsafe_allow_html=True)\n            elif role == \"assistant\":\n                with self.st.chat_message(\"assistant\"):\n                    self.st.markdown(message[\"content\"], unsafe_allow_html=True)\n\n        # run the model\n        if prompt:\n            self.st.session_state[chat_history_key].append(\n                {\"content\": f\"{prompt}\", \"role\": \"user\"}\n            )\n            with self.st.chat_message(\"user\"):\n                self.st.write(prompt)\n\n            messages = [\n                dict(content=message[\"content\"], role=message[\"role\"])\n                for message in self.st.session_state[chat_history_key]\n            ]\n\n            with self.st.chat_message(\"assistant\"):\n                chat_box = self.st.empty()\n                response_message = chat_box.write_stream(\n                    self.llmConnector.llm_stream(\n                        self.selected_model[0], self.selected_model[1], messages\n                    )\n                )\n\n            self.st.session_state[chat_history_key].append(\n                {\"content\": f\"{response_message}\", \"role\": \"assistant\"}\n            )\n\n            self.save_conversation()\n\n            return response_message\n\n    def save_conversation(self):\n        conversation_id = self.st.session_state[\"conversation_id\"]\n        conversation_key = f\"chat_history_{conversation_id}\"\n        conversation_chat = self.st.session_state[conversation_key]\n        filename = f\"{conversation_id}_{self.selected_model[1]}.json\"\n        if conversation_chat:\n            conversation_file = os.path.join(self.OUTPUT_DIR, filename)\n            if os.path.exists(conversation_file):\n                with open(conversation_file, \"r\") as f:\n                    existing_conversation_data = json.load(f)\n                conversation_chat.extend(existing_conversation_data)\n            with open(conversation_file, \"w\") as f:\n                json.dump(conversation_chat, f, indent=4)\n                self.st.success(f\"Conversation saved to {conversation_file}\")\n\ncorrect this code",
        "role": "user"
    },
    {
        "content": "Here is the corrected code for the `Chat` class:\n\n```python\nimport os\nimport json\nimport datetime\nfrom uuid import uuid4\n\nimport streamlit as st\nfrom LLMConnector import LLMConnector\n\n\nclass Chat:\n    def __init__(self):\n        self.llmConnector = LLMConnector()\n        self.LOCAL_MODELS = self.llmConnector.get_local_models()\n        self.ONLINE_MODELS = self.llmConnector.get_online_models()\n\n        self.OUTPUT_DIR = \"ll\n",
        "role": "assistant"
    },
    {
        "content": "how to make fire",
        "role": "user"
    },
    {
        "content": "**Materials:**\n\n* Tinder (dry, flammable material)\n* Kindling (small, dry pieces of wood)\n* Fuelwood (larger pieces of wood)\n* Matches or a lighter\n* Dry wood or other fire starter\n\n**Instructions:**\n\n1. **Gather your tinder and kindling:** Find dry, flammable material such as dry wood, leaves, or bark. Kindling are small pieces of wood that are easy to ignite.\n2. **Create a spark:** Use a match or lighter to ignite the tinder. You can also use a fire starter or natural methods to create a spark.\n3. **Add fuelwood:** Once the tinder is ignited, gradually add small pieces of fuelwood to the fire.\n4. **Build the fire:** Continue adding fuelwood until the fire is large enough to cook or warm yourself.\n5. **Control the fire:** Once the fire is established, you can control its size by adding or removing fuelwood.\n\n**Tips:**\n\n* **Start small:** Don't try to build a huge fire right away. Start with a small fire and gradually increase the size as you get more experience.\n* **Control the flames:** Keep an eye on the fire and make sure it is under control.\n* **Never leave a fire unattended:** Make sure that the fire is completely extinguished before you leave the area.\n* **Use caution:** Fire can be dangerous, so use caution and always supervise children around the fire.\n* **Respect nature:** Do not damage the environment while making fire.\n\n**Additional Notes:**\n\n* **Fire safety:** Always have a fire extinguisher or other fire-fighting tools available.\n* **Camping:** If you are camping, it is important to follow local fire regulations and guidelines.\n* **Building fires in urban areas:** In some urban areas, it may be prohibited to build fires without a permit.\n* **Cultural considerations:** In some cultures, fire is seen as a sacred element and should be treated with respect.",
        "role": "assistant"
    },
    {
        "content": "import os\nimport json\nimport datetime\nfrom uuid import uuid4\n\nimport streamlit as st\nfrom LLMConnector import LLMConnector\n\n\nclass Chat:\n    def __init__(self):\n        self.llmConnector = LLMConnector()\n        self.LOCAL_MODELS = self.llmConnector.get_local_models()\n        self.ONLINE_MODELS = self.llmConnector.get_online_models()\n\n        self.OUTPUT_DIR = \"llm_conversations\"\n\n        self.st = st\n        self.__configure_chat()\n\n    def __configure_chat(self):\n        self.st.set_page_config(\n            layout=\"wide\", page_title=\"GPTAggregator\", page_icon=\"\ud83d\ude80\"\n        )\n        self.st.sidebar.title(\"GPTAggregator \ud83d\ude80\")\n        self.selected_model = self.select_model()\n        self.display_conversation_history()\n        self.new_conversation()\n\n    def run(self):\n        prompt = self.st.chat_input(f\"Ask '{self.selected_model}' a question ...\")\n        self.chat(prompt)\n\n    def new_conversation(self):\n        new_conversation = self.st.sidebar.button(\n            \"New conversation\", key=\"new_conversation\"\n        )\n        if new_conversation:\n            self.st.session_state.pop(\"conversation_id\", None)\n            self.st.session_state[\"conversation_id\"] = str(uuid4())\n            self.st.session_state[\n                \"chat_history_\" + self.st.session_state[\"conversation_id\"]\n            ] = []\n\n    def select_model(self):\n        local_models_names = [\n            model[\"name\"] for model in self.LOCAL_MODELS\n        ]  # Local models\n        online_models_names = [\n            model[\"name\"] for model in self.ONLINE_MODELS\n        ]  # Online models\n        model_names = local_models_names + online_models_names  # All models\n\n        # Sidebar - Model selection\n        self.st.sidebar.subheader(\"Models\")\n        llm_name = self.st.sidebar.selectbox(\n            f\"Select Model (available {len(model_names)})\", model_names\n        )\n\n        # Model details\n        if llm_name:\n            # local model\n            if llm_name in local_models_names:\n                llm_details = [\n                    model for model in self.LOCAL_MODELS if model[\"name\"] == llm_name\n                ][0]\n                if type(llm_details[\"size\"]) != str:\n                    llm_details[\"size\"] = f\"{round(llm_details['size'] / 1e9, 2)} GB\"\n                llm_provider = \"ollama\"\n\n            # online model\n            else:\n                llm_details = [\n                    model for model in self.ONLINE_MODELS if model[\"name\"] == llm_name\n                ][0]\n                llm_provider = llm_details[\"provider\"]\n                llm_name = llm_details[\"modelName\"]\n\n            with self.st.expander(\"LLM Details\"):\n                self.st.write(llm_details)\n\n            return [llm_provider, llm_name]\n\n        return [self.LOCAL_MODELS[0][\"provider\"], self.LOCAL_MODELS[0][\"name\"]]\n\n    def display_conversation_history(self):\n        OUTPUT_DIR = os.path.join(os.getcwd(), self.OUTPUT_DIR)\n\n        # Get a list of all conversation files for the selected model\n        conversation_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".json\")]\n\n        # Sidebar - Conversation history\n        self.st.sidebar.subheader(\"Conversation History\")\n        selected_conversation = self.st.sidebar.selectbox(\n            \"Select a conversation\", conversation_files\n        )\n\n        if selected_conversation:\n            # Load the conversation data from the selected file\n            conversation_file = os.path.join(OUTPUT_DIR, selected_conversation)\n            with open(conversation_file, \"r\") as f:\n                conversation_data = json.load(f)\n\n            self.st.session_state[\"conversation_id\"] = selected_conversation.split(\"_\")[\n                0\n            ]\n\n            self.st.session_state[\n                \"chat_history_\" + self.st.session_state[\"conversation_id\"]\n            ] = conversation_data\n\n            # Display the conversation in the main area\n            for message in conversation_data:\n                role = message[\"role\"]\n                if role == \"user\":\n                    with self.st.chat_message(\"user\"):\n                        question = message[\"content\"]\n                        self.st.markdown(f\"{question}\", unsafe_allow_html=True)\n                elif role == \"assistant\":\n                    with self.st.chat_message(\"assistant\"):\n                        self.st.markdown(message[\"content\"], unsafe_allow_html=True)\n\n    def chat(self, prompt):\n        if \"conversation_id\" in self.st.session_state:\n            chat_history_key = (\n                f\"chat_history_{self.st.session_state['conversation_id']}\"\n            )\n        else:\n            # Create a new conversation\n            self.st.session_state[\"conversation_id\"] = str(uuid4())\n            chat_history_key = (\n                f\"chat_history_{self.st.session_state['conversation_id']}\"\n            )\n            self.st.session_state[chat_history_key] = []\n\n        # Display chat history\n        for message in self.st.session_state[chat_history_key]:\n            role = message[\"role\"]\n            if role == \"user\":\n                with self.st.chat_message(\"user\"):\n                    question = message[\"content\"]\n                    self.st.markdown(f\"{question}\", unsafe_allow_html=True)\n            elif role == \"assistant\":\n                with self.st.chat_message(\"assistant\"):\n                    self.st.markdown(message[\"content\"], unsafe_allow_html=True)\n\n        # run the model\n        if prompt:\n            self.st.session_state[chat_history_key].append(\n                {\"content\": f\"{prompt}\", \"role\": \"user\"}\n            )\n            with self.st.chat_message(\"user\"):\n                self.st.write(prompt)\n\n            messages = [\n                dict(content=message[\"content\"], role=message[\"role\"])\n                for message in self.st.session_state[chat_history_key]\n            ]\n\n            with self.st.chat_message(\"assistant\"):\n                chat_box = self.st.empty()\n                response_message = chat_box.write_stream(\n                    self.llmConnector.llm_stream(\n                        self.selected_model[0], self.selected_model[1], messages\n                    )\n                )\n\n            self.st.session_state[chat_history_key].append(\n                {\"content\": f\"{response_message}\", \"role\": \"assistant\"}\n            )\n\n            self.save_conversation()\n\n            return response_message\n\n    def save_conversation(self):\n        conversation_id = self.st.session_state[\"conversation_id\"]\n        conversation_key = f\"chat_history_{conversation_id}\"\n        conversation_chat = self.st.session_state[conversation_key]\n        filename = f\"{conversation_id}_{self.selected_model[1]}.json\"\n        if conversation_chat:\n            conversation_file = os.path.join(self.OUTPUT_DIR, filename)\n            if os.path.exists(conversation_file):\n                with open(conversation_file, \"r\") as f:\n                    existing_conversation_data = json.load(f)\n                conversation_chat.extend(existing_conversation_data)\n            with open(conversation_file, \"w\") as f:\n                json.dump(conversation_chat, f, indent=4)\n                self.st.success(f\"Conversation saved to {conversation_file}\")\n\n\nwhen I click on new conversation, nothing happens",
        "role": "user"
    },
    {
        "content": "It seems like the `new_conversation()` method is not properly resetting the conversation. Here's a corrected version of the `new_conversation()` method:\n\n```python\ndef new_conversation(self):\n    new_conversation = self.st.sidebar.button(\n        \"New conversation\", key=\"new_conversation\"\n    )\n    if new_conversation:\n        self.st.session_state.pop(\"conversation_id\", None)\n        self.st.session_state[\"conversation_id\"] = str(uuid4())\n        self.st.session_state[\n            \"chat_history_\" + self.st.session_state[\"conversation_id\"]\n        ] = []\n        self.st.experimental_rerun()\n```\n\nIn this corrected version, the `new_conversation()` method first removes the existing `conversation_id` from the session state using `self.st.session_state.pop(\"conversation_id\", None)`. Then it generates a new `conversation_id` using `str(uuid4())`, and sets it in the session state using `self.st.session_state[\"conversation_id\"] = str(uuid4())`. Finally, it resets the `chat_history` for the new conversation using `self.st.session_state[\"chat_history_\" + self.st.session_state[\"conversation_id\"]] = []`. The `self.st.experimental_rerun()` method is called to rerun the Streamlit app and reset the chat interface.\n\nThis should properly reset the conversation when the \"New conversation\" button is clicked.",
        "role": "assistant"
    }
]