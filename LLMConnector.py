import os
import json

import ollama
from openai import OpenAI
from anthropic import Anthropic
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage


class LLMConnector:
    """
    A connector class designed to interface with various large language models (LLMs) through their respective APIs.
    Supports local and online model querying, as well as streaming responses from these models.
    """

    def __init__(self):
        """
        Initializes the LLMConnector with client instances for each supported API and loads the configuration for online models.
        """
        self.ollama_client = ollama  # Client for interacting with local Ollama models.
        self.openai_client = OpenAI()  # Client for OpenAI's API.
        self.anthropic_client = Anthropic()  # Client for Anthropic's API.
        self.mistralai_client = MistralClient(
            api_key=os.getenv("MISTRALAI_API_KEY")
        )  # Fetches the MistralAI API key from environment variables.
        self.online_models = json.load(
            open("models.json")
        )  # Loads the online model configurations from a JSON file.

    def get_local_models(self):
        """
        Fetches and returns a list of local models available through the Ollama client.

        Returns:
            list: A list of model names available through the Ollama client.
        """
        return self.ollama_client.list()["models"]

    def get_online_models(self):
        """
        Fetches and returns a list of online models available through the Ollama, OpenAI, Anthropic, and MistralAI clients.

        Returns:
            dict: A dictionary of online model configurations.
        """
        return self.online_models

    def llm_stream(self, provider, model_name, messages):
        """
        Streams responses from the specified language model in real-time. This method handles interactions with different LLM providers by yielding messages as they become available.

        Args:
            provider (str): The name of the LLM provider (ollama, openai, anthropic, mistralai).
            model_name (str): The name or identifier of the model to use for generating responses.
            messages (list): A list of messages to send to the model. Can include questions or prompts.

        Yields:
            str: A response message generated by the language model.
        """

        # Ollama API - Local LLM
        if provider == "ollama":
            stream = self.ollama_client.chat(model_name, messages, stream=True)
            for chunk in stream:
                yield chunk["message"]["content"]

        # OpenAI API
        elif provider == "openai":
            stream = self.openai_client.chat.completions.create(
                model=model_name,
                messages=messages,
                stream=True,
            )
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    yield content

        # Anthropic API
        elif provider == "anthropic":
            with self.anthropic_client.messages.stream(
                max_tokens=1024,
                model=model_name,
                messages=messages,
            ) as stream:
                for text in stream.text_stream:
                    yield text

        # MistralAI API
        elif provider == "mistralai":
            stream = self.mistralai_client.chat_stream(
                model=model_name,
                messages=messages,
            )
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    yield content
